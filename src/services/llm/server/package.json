{
  "name": "llm-server",
  "version": "1.0.0",
  "description": "Local LLM inference server for O-intecOptima",
  "main": "api.ts",
  "type": "module",
  "scripts": {
    "start": "node --loader ts-node/esm api.ts",
    "dev": "nodemon --exec node --loader ts-node/esm api.ts",
    "build": "tsc"
  },
  "dependencies": {
    "@xenova/transformers": "^2.15.0",
    "cors": "^2.8.5",
    "express": "^4.18.2"
  },
  "devDependencies": {
    "@types/cors": "^2.8.17",
    "@types/express": "^4.17.21",
    "@types/node": "^20.11.0",
    "nodemon": "^3.0.2",
    "ts-node": "^10.9.2",
    "typescript": "^5.3.3"
  }
} 